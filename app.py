import gradio as gr
import torch
from transformers import AutoModel, AutoTokenizer, AutoConfig
from PIL import Image

# åŠ è½½æ¨¡å‹éƒ¨åˆ†
# modelscope download livehouse/SanguoVLM --local_dir /home/xlab-app-center/SanguoVLM
#æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('livehouse/SanguoVLM')

path = model_dir

device_map = None
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    load_in_8bit=False,
    low_cpu_mem_usage=True,
    use_flash_attn=False,
    trust_remote_code=True,
    device_map="auto" if device_map is None else device_map
).eval()
tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)

# é¢„å¤„ç†å›¾ç‰‡å‡½æ•°
def preprocess(image, max_num=12, image_size=448):
    from torchvision import transforms as T
    from torchvision.transforms.functional import InterpolationMode
    transform = T.Compose([
        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        T.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    pixel_values = transform(image)
    pixel_values = pixel_values.unsqueeze(0).to(torch.bfloat16).cuda()
    return pixel_values

# æ¨ç†å‡½æ•°
def predict(image, message, history):
    if image is None:
        return history + [("è¯·ä¸Šä¼ ä¸€å¼ å›¾ç‰‡", "")]

    pixel_values = preprocess(image)
    
    if history is None or len(history) == 0:
        user_input = f"<image>\n{message}"
        response, new_history = model.chat(
            tokenizer, pixel_values, user_input,
            generation_config={"max_new_tokens": 1024, "do_sample": True},
            history=None, return_history=True
        )
    else:
        response, new_history = model.chat(
            tokenizer, pixel_values, message,
            generation_config={"max_new_tokens": 1024, "do_sample": True},
            history=history, return_history=True
        )
    
    return new_history

# Gradioç•Œé¢
with gr.Blocks(title="InternVL3-9B-sft Web Demo") as demo:
    gr.Markdown("<h1 align='center'>ğŸ“· InternVL3-9B-sft Chat Demo</h1>")
    gr.Markdown("<p align='center'>âš ï¸ åŠ¡å¿…ä¸Šä¼ <strong>æ­£æ–¹å½¢å›¾ç‰‡</strong>ï¼Œå¦åˆ™äººè„¸å‹ç¼©ï¼Œè¯†åˆ«ä¸å‡†ã€‚æ¨èåˆ†è¾¨ç‡ä¸º <code>448Ã—448</code> å°ºå¯¸ä»ç„¶èƒ½çœ‹æ¸…äººè„¸çš„å›¾ç‰‡ã€‚</p>")
    
    with gr.Row():
        with gr.Column(scale=1):
            image_input = gr.Image(type="pil", label="ä¸Šä¼ ä¸€å¼ å›¾ç‰‡")
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(height=600, label="å¯¹è¯")
            message_input = gr.Textbox(placeholder="è¯·è¾“å…¥ä½ çš„é—®é¢˜...", label="æé—®")
            send_btn = gr.Button("å‘é€")
    
    state = gr.State([])  # ä¿å­˜history

    def on_send(image, message, history):
        if message.strip() == "":
            return history
        history = predict(image, message, history)
        return history

    send_btn.click(
        fn=on_send,
        inputs=[image_input, message_input, state],
        outputs=[chatbot]
    )
    message_input.submit(
        fn=on_send,
        inputs=[image_input, message_input, state],
        outputs=[chatbot]
    )

demo.launch(server_name="127.0.0.1", server_port=6000, share=True)